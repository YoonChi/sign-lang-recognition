{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b64ff47-4daf-4d9d-97a3-ec0f00cccb90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5125 images belonging to 26 classes.\n",
      "Found 5125 images belonging to 26 classes.\n",
      "### plotting the images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2160x1440 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 64, 64, 3)\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 1.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]]\n",
      "### initialized model\n",
      "### begin layering\n",
      "### flatten images\n",
      "### compile with optimizer\n",
      "Epoch 1/10\n",
      "513/513 [==============================] - 26s 50ms/step - loss: 0.6173 - accuracy: 0.8970 - val_loss: 0.0204 - val_accuracy: 0.9967 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "513/513 [==============================] - 26s 50ms/step - loss: 0.0095 - accuracy: 0.9984 - val_loss: 0.0032 - val_accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "513/513 [==============================] - 26s 50ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.0015 - val_accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "513/513 [==============================] - 25s 48ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 9.9972e-04 - val_accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "513/513 [==============================] - 25s 48ms/step - loss: 9.3115e-04 - accuracy: 1.0000 - val_loss: 7.5405e-04 - val_accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 6/10\n",
      "513/513 [==============================] - 26s 50ms/step - loss: 7.0945e-04 - accuracy: 1.0000 - val_loss: 6.0526e-04 - val_accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 7/10\n",
      "513/513 [==============================] - 23s 46ms/step - loss: 5.7370e-04 - accuracy: 1.0000 - val_loss: 4.9952e-04 - val_accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 8/10\n",
      "513/513 [==============================] - 25s 49ms/step - loss: 4.7872e-04 - accuracy: 1.0000 - val_loss: 4.2870e-04 - val_accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 9/10\n",
      "513/513 [==============================] - 26s 50ms/step - loss: 4.1975e-04 - accuracy: 1.0000 - val_loss: 3.9610e-04 - val_accuracy: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 10/10\n",
      "513/513 [==============================] - 24s 47ms/step - loss: 3.9076e-04 - accuracy: 1.0000 - val_loss: 3.7068e-04 - val_accuracy: 1.0000 - lr: 5.0000e-04\n",
      "loss of 9.429263445781544e-05; accuracy of 100.0%\n",
      "### printing scores\n",
      "loss of 0.0003879569412674755; accuracy of 100.0%\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_3 (Conv2D)           (None, 62, 62, 32)        896       \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 31, 31, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 31, 31, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPooling  (None, 15, 15, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 15, 15, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPooling  (None, 7, 7, 128)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 6272)              0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 64)                401472    \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 128)               8320      \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 26)                3354      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 522,906\n",
      "Trainable params: 522,906\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "finished\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, Flatten, BatchNormalization, Conv2D, MaxPool2D, Dropout\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from keras.metrics import categorical_crossentropy\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import itertools\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "import cv2\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "\n",
    "train_path = '/Users/yooniiechi/Desktop/AI/AI_PROJECT-ASL/gesture/train'\n",
    "test_path = '/Users/yooniiechi/Desktop/AI/AI_PROJECT-ASL/gesture/test'\n",
    "\n",
    "train_batches = ImageDataGenerator(preprocessing_function=tf.keras.applications.vgg16.preprocess_input).flow_from_directory(directory=train_path, target_size=(64,64), class_mode='categorical', batch_size=10,shuffle=True)\n",
    "test_batches = ImageDataGenerator(preprocessing_function=tf.keras.applications.vgg16.preprocess_input).flow_from_directory(directory=test_path, target_size=(64,64), class_mode='categorical', batch_size=10, shuffle=True)\n",
    "\n",
    "imgs, labels = next(train_batches)\n",
    "\n",
    "\n",
    "#Plotting the images...\n",
    "print(\"### plotting the images...\")\n",
    "def plotImages(images_arr):\n",
    "    fig, axes = plt.subplots(1, 10, figsize=(30,20))\n",
    "    axes = axes.flatten()\n",
    "    for img, ax in zip( images_arr, axes):\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plotImages(imgs)\n",
    "print(imgs.shape)\n",
    "print(labels)\n",
    "\n",
    "print(\"### initialized model\")\n",
    "model = Sequential()\n",
    "\n",
    "print(\"### begin layering\")\n",
    "model.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu', input_shape=(64,64,3))) #padding ≠ same, as output value size ≠ input volume size\n",
    "\n",
    "model.add(MaxPool2D(pool_size=(2, 2), strides=2)) #downsampled\n",
    "\n",
    "model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding = 'same'))\n",
    "model.add(MaxPool2D(pool_size=(2, 2), strides=2))\n",
    "\n",
    "model.add(Conv2D(filters=128, kernel_size=(3, 3), activation='relu', padding = 'same'))\n",
    "model.add(MaxPool2D(pool_size=(2, 2), strides=2))\n",
    "\n",
    "print(\"### flatten images\")\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(64,activation =\"relu\"))\n",
    "model.add(Dense(128,activation =\"relu\"))\n",
    "#model.add(Dropout(0.2))\n",
    "model.add(Dense(128,activation =\"relu\"))\n",
    "#model.add(Dropout(0.3))\n",
    "model.add(Dense(26,activation =\"softmax\"))\n",
    "\n",
    "# In[23]:\n",
    "\n",
    "# model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "# reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=1, min_lr=0.0001)\n",
    "# early_stop = EarlyStopping(monitor='val_loss', min_delta=0, patience=2, verbose=0, mode='auto')\n",
    "\n",
    "print(\"### compile with optimizer\")\n",
    "model.compile(optimizer=SGD(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=1, min_lr=0.0005)\n",
    "early_stop = EarlyStopping(monitor='val_loss', min_delta=0, patience=2, verbose=0, mode='auto')\n",
    "\n",
    "model.fit(train_batches, epochs=6, callbacks=[reduce_lr, early_stop],  validation_data = test_batches)#, checkpoint])\n",
    "imgs, labels = next(train_batches) # For getting next batch of imgs...\n",
    "\n",
    "imgs, labels = next(test_batches) # For getting next batch of imgs...\n",
    "scores = model.evaluate(imgs, labels, verbose=0)\n",
    "print(f'{model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
    "\n",
    "model.save('best_model.h5')\n",
    "\n",
    "#print(history2.history)\n",
    "\n",
    "imgs, labels = next(test_batches)\n",
    "\n",
    "#model = keras.models.load_model(r\"best_model_dataflair3.h5\")\n",
    "\n",
    "scores = model.evaluate(imgs, labels, verbose=0)\n",
    "\n",
    "print(\"### printing scores\")\n",
    "print(f'{model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
    "\n",
    "model.summary()\n",
    "\n",
    "scores #[loss, accuracy] on test data...\n",
    "model.metrics_names\n",
    "\n",
    "# word_dict = {0:'One',1:'Ten',2:'Two',3:'Three',4:'Four',5:'Five',6:'Six',7:'Seven',8:'Eight',9:'Nine'}\n",
    "\n",
    "# predictions = model.predict(imgs, verbose=0)\n",
    "# print(\"predictions on a small set of test data--\")\n",
    "# print(\"\")\n",
    "# for ind, i in enumerate(predictions):\n",
    "#     print(word_dict[np.argmax(i)], end='   ')\n",
    "\n",
    "# plotImages(imgs)\n",
    "# print('Actual labels')\n",
    "# for i in labels:\n",
    "#     print(word_dict[np.argmax(i)], end='   ')\n",
    "\n",
    "# print(\"images shape\", imgs.shape)\n",
    "\n",
    "\n",
    "# history2.history\n",
    "\n",
    "print(\"finished\")\n",
    "# %%\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
